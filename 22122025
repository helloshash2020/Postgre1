From what youâ€™ve described, the behavior youâ€™re seeing is common on large PostgreSQL systems with huge pages enabled, and 100% swap usage does not automatically mean a problem. Letâ€™s break it down clearly and tie it to your deadlock incident.
1. Key facts from your setup
PostgreSQL: 14.19
DB size: ~4 TB
RAM: 108 GB
Swap: 8 GB (vmdk)
Huge Pages: Enabled
Swap usage: ~85% average, often 100%
Deadlock: Occurred earlier, fixed at application level
2. Why swap is almost always full (even when system looks healthy)
ğŸ”¹ Huge Pages + Linux memory behavior
When huge pages are enabled:
PostgreSQL shared memory (shared_buffers) is locked into RAM
That memory cannot be swapped
Linux then pushes rarely-used anonymous memory (glibc, idle backend memory, caches) to swap early
ğŸ‘‰ Result:
Swap gets filled, but PostgreSQL critical memory stays in RAM
This is expected and acceptable behavior.
3. Why 8 GB swap fills up quickly on 108 GB RAM
Linux treats swap as:
A pressure relief valve
A place to park cold pages
With:
Many backend processes
Long-running connections
Monitoring agents, cron jobs, shell sessions
Even if each process swaps 10â€“50 MB, swap fills fast.
Important:
What matters is swap activity, not swap fullness.
4. How to check if swap usage is actually harmful
âœ… Healthy (your likely case)
si / so (swap in/out) â‰ˆ 0
No sustained CPU wait (wa)
No PostgreSQL query latency spikes
No oom-killer events
No major pg_stat_activity stalls
ğŸš¨ Problematic
Constant swap-in/swap-out
CPU high in wa
Queries freezing under load
Kernel logs with memory pressure
5. Deadlocks are not related to swap usage
Your deadlock issue:
Was application-level
Likely due to lock ordering or transaction design
Resolved by code change (good)
â— Deadlocks are logical concurrency issues, not memory/swap problems.
6. What you SHOULD verify (recommended checks)
ğŸ” OS-level
Copy code
Bash
vmstat 1 5
si / so should be near 0
Copy code
Bash
free -h
Check available memory, not â€œusedâ€
Copy code
Bash
swapon --show
ğŸ” PostgreSQL-level
Copy code
Sql
SHOW shared_buffers;
SHOW huge_pages;
SHOW work_mem;
SHOW max_connections;
Also:
Copy code
Sql
SELECT count(*) FROM pg_stat_activity;
High idle connections â†’ more swap usage.
7. Practical tuning recommendations (safe)
âœ… Keep huge pages enabled
Youâ€™re doing the right thing for:
Large RAM
4 TB DB
Stable performance
âš™ï¸ Increase swap size (recommended)
8 GB swap is too small for a 108 GB RAM system.
Suggested:
Swap = 16â€“32 GB
Even if unused, it prevents memory pressure spikes
This is especially important during:
Autovacuum bursts
Checkpoints
Backup jobs
Kernel memory fragmentation
âš™ï¸ Adjust swappiness
Copy code
Bash
sysctl vm.swappiness=10
Persist in:
Copy code
Bash
/etc/sysctl.conf
This:
Keeps PostgreSQL memory in RAM
Still allows emergency swap usage
âš™ï¸ Review connection count
If max_connections is very high:
Each backend consumes private memory
Consider PgBouncer (transaction pooling)
8. Bottom line (executive summary)
âœ” Swap being 85â€“100% used is normal with huge pages
âœ” No link between swap usage and deadlocks
âœ” Problem only if swap I/O is active
âœ” Increase swap to 16â€“32 GB for safety
âœ” Keep huge pages enabled
âœ” Monitor swap-in/out, not swap size
If you want, I can:
Review your PostgreSQL memory parameters
Suggest exact huge_pages calculation
Help you size swap optimally for PostgreSQL 14
Correlate swap usage with autovacuum/checkpoints
Just share:
shared_buffers
work_mem
max_connections
vmstat 1 5 output



Hereâ€™s a clear, production-oriented comparison of
log_lock_waits = on vs log_lock_waits = off in PostgreSQL 14.19, focused on what actually gets logged and when you should use each.
ğŸ” What PostgreSQL means by a â€œlock waitâ€
A lock wait is logged only if:
a session waits longer than deadlock_timeout
AND log_lock_waits = on
Deadlocks are detected during this same mechanism.
âš™ï¸ log_lock_waits = off (DEFAULT)
Behavior
âŒ No lock waits are logged
âŒ Deadlocks are NOT logged
âœ” PostgreSQL still detects and resolves deadlocks internally
âœ” Client still receives error SQLSTATE 40P01
What you see in logs
Nothing related to locks:
Copy code

(no lock-related entries)
When to use
Extremely log-sensitive environments
You rely only on application-side retry & logging
You donâ€™t need server-side deadlock visibility
âš ï¸ Risk: Deadlocks can occur silently unless the app logs them.
âš™ï¸ log_lock_waits = on
Behavior
âœ” Logs long lock waits (> deadlock_timeout)
âœ” Logs deadlocks
âœ” Adds minimal overhead
âŒ Can generate noise if deadlock_timeout is large
What you see in logs
Long lock wait (not a deadlock):
Copy code

LOG: process 12345 still waiting for ShareLock on transaction 67890 after 1000.000 ms
Deadlock (this is what you care about):
Copy code

ERROR: deadlock detected
DETAIL: Process 111 waits for ExclusiveLock on relation 222; blocked by process 333.
When to use
You want server-side evidence of deadlocks
You want minimal but critical lock diagnostics
Recommended for most production systems
ğŸ¯ Key difference (simple table)
Feature
off
on
Deadlock logged
âŒ No
âœ… Yes
Long lock waits logged
âŒ No
âœ… Yes (> deadlock_timeout)
Overhead
None
Very low
Log noise
None
Low (if tuned)
RCA friendly
âŒ
âœ…
ğŸ§  Important nuance (very relevant for you)
PostgreSQL cannot log only deadlocks.
The only way to capture deadlocks is:
Copy code
Conf
log_lock_waits = on
Then control noise using:
Copy code
Conf
deadlock_timeout = 1s
Lower timeout = faster detection
Higher timeout = fewer lock-wait logs
âœ… Recommended setting for your case
Given:
Deadlock already fixed in application
You want only problematic locks
You want minimal logs
ğŸ‘‰ Best compromise
Copy code
Conf
log_lock_waits = on
deadlock_timeout = 1s
log_min_messages = error
This ensures:
Deadlocks are always logged
Normal locks stay quiet
No restart needed
ğŸ”š Final takeaway
off = blind to deadlocks
on  = safety net with low noise
If you want, I can next:
Show exact log lines youâ€™ll see
Explain how deadlock_timeout impacts OLTP systems
Help create a runbook for lock incidents